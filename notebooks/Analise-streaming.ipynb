{"cells":[{"cell_type":"markdown","source":["### Streaming dos dados JSON\nPara o streaming dos dados foi configurada uma arquitetura Kappa, onde os dados são ingeridos pelo Kafka, processados no Spark e armazenados diretamente no Delta Lake. Por esse motivo este notebook foi criado em um cluster <a href=\"community.cloud.databricks.com/\" target=\"_blank\">**databricks community**</a>, sua versão gratuita. Para acesso/teste desta solução posso receber a chave pública de vocês para liberar o acesso ao servidor kafka, onde pode ser feito o teste da ingestão dos dados através do tópico Kafka. Já a execução desse notebook pode ser feita em qualquer cluster criado no databricks community.\n\n[![kappa](https://storage.googleapis.com/repo-files/kappa.png)]()\n\n## Kafka\nUma instância Ubuntu foi criada no GCP e possui o confluent instalado. Asim que os arquivos json são recebidos, um tópico do kafka ingere os dados e os deixa disponíveis para serem processados pelo Spark, conforme o código contido neste notebook.\n\n## Spark\nOs dados disponibilizados pelo Kafka são recebidos em binário, são lidos e convertidos em string e posteriormente processados no spark.\n\n## Delta\nApós o processamento feito no Spark, os dados são armazenados no Delta Lake, onde podemos integrar SQL diretamente com o streaming do dado."],"metadata":{}},{"cell_type":"markdown","source":["## Importando todas as bibliotecas necessárias"],"metadata":{}},{"cell_type":"code","source":["import pyspark\n\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.streaming import *\n\n\nsc = SparkContext.getOrCreate()\nspark = SparkSession(sc)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["## Testando a conexão ao servidor e ao broker do kafka"],"metadata":{}},{"cell_type":"code","source":["%sh telnet 35.238.128.197 9092"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["## Definindo as variáveis\nAs variáveis abaixo serão utilizadas na leitura do Kafka pelo Spark. São elas referente a string de conexão do broker, do schema registry e dos tópicos do Kafka."],"metadata":{}},{"cell_type":"code","source":["#kafka\nkafkaBrokers = \"35.238.128.197:9092\"\n\n#schema registry\nschemaRegistryAddr = \"http://35.238.128.197:8081\"\n\n#topic\nspooldir_topic = \"spooldir2009-json-topic, spooldir2010-json-topic, spooldir2011-json-topic, spooldir2012-json-topic\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["## Definindo o schema dos dados\nO dado recebido do Kafka é do tipo binário. Aqui é difinido o schema que será associado ao dado no Spark, o que converterá o dado em uma estrutura do tipo Spark Struct"],"metadata":{}},{"cell_type":"code","source":["jsonSchema = StructType([ StructField('dropoff_datetime', TimestampType(), True), \\\n                         StructField('dropoff_latitude', DoubleType(), True), \\\n                         StructField('dropoff_longitude', DoubleType(), True), \\\n                         StructField('fare_amount', DoubleType(), True), \\\n                         StructField('passenger_count', LongType(), True), \\\n                         StructField('payment_type', StringType(), True), \\\n                         StructField('pickup_datetime', TimestampType(), True), \\\n                         StructField('pickup_latitude', DoubleType(), True), \\\n                         StructField('pickup_longitude', DoubleType(), True), \\\n                         StructField('rate_code', StringType(), True), \\\n                         StructField('store_and_fwd_flag', LongType(), True), \\\n                         StructField('surcharge', DoubleType(), True), \\\n                         StructField('tip_amount', DoubleType(), True), \\\n                         StructField('tolls_amount', DoubleType(), True), \\\n                         StructField('total_amount', DoubleType(), True), \\\n                         StructField('trip_distance', DoubleType(), True), \\\n                         StructField('vendor_id', StringType(), True)])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["## Leitura do tópico kafka\nEsse é o primeiro passo para especificar a localização do Kafka e quais tópicos serão lidos. O Spark permite a leitura de mais de um tópico simultaneamente. Neste caso serão lidos quatro tópicos, cada um referente a um arquivo (2009, 2010, 2011, 2012)."],"metadata":{}},{"cell_type":"code","source":["df_kafka_nyctaxitrips = spark.readStream \\\n                        .format(\"kafka\") \\\n                        .option(\"kafka.bootstrap.servers\", kafkaBrokers) \\\n                        .option(\"subscribe\", spooldir_topic) \\\n                        .option(\"startingOffsets\", \"earliest\") \\\n                        .load()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["## Convertendo o valor binário lido do kafka para string\nOs bytes do registro do Kafka representam Strings UTF8. Neste caso basta converter os dados binários para o tipo desejado."],"metadata":{}},{"cell_type":"code","source":["df = df_kafka_nyctaxitrips.selectExpr(\"CAST(value AS STRING)\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["Os dados estão chegando no formato JSON. Para associar o schema correto, é utilizada a função nativa do Spark `from_json` junto ao schema já definido previamente."],"metadata":{}},{"cell_type":"code","source":["dfnyctaxitrips = df.select(from_json(df.value, jsonSchema).alias(\"json\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["## Validando se o dataframe está transmitindo dados."],"metadata":{}},{"cell_type":"code","source":["df_kafka_nyctaxitrips.isStreaming"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[7]: True</div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["## Carga dos dados de streaming no delta lake\nOs dados em streaming serão escritos no Delta Lake, onde poderão ser manipulados."],"metadata":{}},{"cell_type":"code","source":["dfnyctaxitrips.writeStream \\\n                     .format(\"delta\") \\\n                     .outputMode(\"append\") \\\n                     .option(\"checkpointLocation\", \"/home/fabricio_dutra87/ingest-nyctaxitrips\") \\\n                     .start(\"/home/fabricio_dutra87/ingest-nyctaxitrips\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[8]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7fb37cfdc2e8&gt;</div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["## Queries em SQL diretamente no Delta Lake após a ingestão dos dados\nComo exemplo, foi executada diretamente no Delta Lake a consulta referente a questão número 4 do teste técnico:\n- Faça um gráfico de série temporal contando a quantidade de gorjetas de cada dia, nos últimos 3 meses de 2012"],"metadata":{}},{"cell_type":"code","source":["%sql\n\nselect to_date(substr(json.dropoff_datetime, 1,10)) as day,\n                       sum(json.tip_amount) as sum_tips\n                       from delta.`/home/fabricio_dutra87/ingest-nyctaxitrips`\n                       where json.dropoff_datetime >= ('2012-10-01')\n                       group by day\n                       order by day asc"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>day</th><th>sum_tips</th></tr></thead><tbody><tr><td>2012-10-01</td><td>1438.2900000000004</td></tr><tr><td>2012-10-02</td><td>1444.7399999999998</td></tr><tr><td>2012-10-03</td><td>1335.76</td></tr><tr><td>2012-10-04</td><td>1407.6700000000005</td></tr><tr><td>2012-10-05</td><td>1392.94</td></tr><tr><td>2012-10-06</td><td>1362.4500000000003</td></tr><tr><td>2012-10-07</td><td>1347.4699999999998</td></tr><tr><td>2012-10-08</td><td>1307.6199999999997</td></tr><tr><td>2012-10-09</td><td>1396.24</td></tr><tr><td>2012-10-10</td><td>1443.66</td></tr><tr><td>2012-10-11</td><td>1366.14</td></tr><tr><td>2012-10-12</td><td>1410.2200000000005</td></tr><tr><td>2012-10-13</td><td>1314.9999999999995</td></tr><tr><td>2012-10-14</td><td>1239.5000000000005</td></tr><tr><td>2012-10-15</td><td>1302.22</td></tr><tr><td>2012-10-16</td><td>1341.9700000000003</td></tr><tr><td>2012-10-17</td><td>1599.89</td></tr><tr><td>2012-10-18</td><td>1347.52</td></tr><tr><td>2012-10-19</td><td>1361.7800000000002</td></tr><tr><td>2012-10-20</td><td>1364.45</td></tr><tr><td>2012-10-21</td><td>1550.5399999999995</td></tr><tr><td>2012-10-22</td><td>1363.0099999999998</td></tr><tr><td>2012-10-23</td><td>1420.9699999999996</td></tr><tr><td>2012-10-24</td><td>1232.9699999999998</td></tr><tr><td>2012-10-25</td><td>1255.37</td></tr><tr><td>2012-10-26</td><td>1397.0999999999997</td></tr><tr><td>2012-10-27</td><td>1221.6599999999996</td></tr><tr><td>2012-10-28</td><td>16.27</td></tr></tbody></table></div>"]}}],"execution_count":21}],"metadata":{"name":"Analise-streaming","notebookId":3918044930528850},"nbformat":4,"nbformat_minor":0}
